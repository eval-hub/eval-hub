{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Eval Hub API Examples\"\n",
        "subtitle: \"Comprehensive guide to using the Evaluation Hub REST API\"\n",
        "author: \"Evaluation Service Team\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    code-fold: false\n",
        "    theme: cosmo\n",
        "  ipynb:\n",
        "    output-file: api_examples.ipynb\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Eval Hub API Examples\n",
        "\n",
        "This notebook demonstrates how to interact with the Evaluation Hub REST API running on `localhost:8000`.\n",
        "\n",
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from uuid import uuid4\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"http://localhost:8000\"\n",
        "API_BASE = f\"{BASE_URL}/api/v1\"\n",
        "\n",
        "# Helper function for pretty printing JSON responses\n",
        "def print_json(data):\n",
        "    print(json.dumps(data, indent=2, default=str))\n",
        "\n",
        "# Helper function for API requests\n",
        "def api_request(method: str, endpoint: str, **kwargs) -> requests.Response:\n",
        "    \"\"\"Make an API request with proper error handling.\"\"\"\n",
        "    url = f\"{API_BASE}{endpoint}\"\n",
        "    response = requests.request(method, url, **kwargs)\n",
        "\n",
        "    print(f\"{method.upper()} {url}\")\n",
        "    print(f\"Status: {response.status_code}\")\n",
        "\n",
        "    if response.headers.get('content-type', '').startswith('application/json'):\n",
        "        print(\"Response:\")\n",
        "        print_json(response.json())\n",
        "    else:\n",
        "        print(f\"Response: {response.text}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Health Check\n",
        "\n",
        "First, let's verify the service is running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/health\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    health_data = response.json()\n",
        "    print(\"‚úÖ Service is healthy!\")\n",
        "    print(f\"Version: {health_data['version']}\")\n",
        "    print(f\"Uptime: {health_data['uptime_seconds']:.1f} seconds\")\n",
        "else:\n",
        "    print(\"‚ùå Service is not responding correctly\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Provider Management\n",
        "\n",
        "### List All Providers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/providers\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    providers_data = response.json()\n",
        "    print(f\"Found {providers_data['total_providers']} providers:\")\n",
        "    for provider in providers_data['providers']:\n",
        "        print(f\"  - {provider['provider_name']} ({provider['provider_id']})\")\n",
        "        print(f\"    Type: {provider['provider_type']}\")\n",
        "        print(f\"    Benchmarks: {provider['benchmark_count']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Specific Provider Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get details for the lm_evaluation_harness provider\n",
        "provider_id = \"lm_evaluation_harness\"\n",
        "response = api_request(\"GET\", f\"/providers/{provider_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    provider = response.json()\n",
        "    print(f\"Provider: {provider['provider_name']}\")\n",
        "    print(f\"Description: {provider['description']}\")\n",
        "    print(f\"Number of benchmarks: {len(provider['benchmarks'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Discovery\n",
        "\n",
        "### List All Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/benchmarks\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    benchmarks_data = response.json()\n",
        "    print(f\"Total benchmarks available: {benchmarks_data['total_count']}\")\n",
        "\n",
        "    # Show first 5 benchmarks\n",
        "    for benchmark in benchmarks_data['benchmarks'][:5]:\n",
        "        print(f\"  - {benchmark['name']} ({benchmark['benchmark_id']})\")\n",
        "        print(f\"    Category: {benchmark['category']}\")\n",
        "        print(f\"    Provider: {benchmark['provider_id']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter Benchmarks by Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/benchmarks\", params={\"category\": \"math\"})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    math_benchmarks = response.json()\n",
        "    print(f\"Math benchmarks: {math_benchmarks['total_count']}\")\n",
        "    for benchmark in math_benchmarks['benchmarks']:\n",
        "        print(f\"  - {benchmark['name']}: {benchmark['description']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Provider-Specific Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "provider_id = \"lm_evaluation_harness\"\n",
        "response = api_request(\"GET\", f\"/providers/{provider_id}/benchmarks\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    benchmarks = response.json()\n",
        "    print(f\"Benchmarks for {provider_id}: {len(benchmarks)}\")\n",
        "\n",
        "    # Group by category\n",
        "    categories = {}\n",
        "    for benchmark in benchmarks:\n",
        "        category = benchmark['category']\n",
        "        if category not in categories:\n",
        "            categories[category] = []\n",
        "        categories[category].append(benchmark['name'])\n",
        "\n",
        "    for category, names in categories.items():\n",
        "        print(f\"\\n{category.title()}: {len(names)} benchmarks\")\n",
        "        print(f\"  Examples: {', '.join(names[:3])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collections\n",
        "\n",
        "### List Available Collections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/collections\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    collections = response.json()\n",
        "    print(f\"Available collections: {collections['total_collections']}\")\n",
        "\n",
        "    for collection in collections['collections']:\n",
        "        print(f\"\\nüìÅ {collection['name']} ({collection['collection_id']})\")\n",
        "        print(f\"   Description: {collection['description']}\")\n",
        "        print(f\"   Benchmarks: {len(collection['benchmarks'])}\")\n",
        "        for benchmark_ref in collection['benchmarks'][:3]:  # Show first 3\n",
        "            print(f\"     - {benchmark_ref['provider_id']}::{benchmark_ref['benchmark_id']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Management\n",
        "\n",
        "### List All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/models\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    models_data = response.json()\n",
        "    print(f\"Total models: {models_data['total_models']}\")\n",
        "    print(f\"Runtime models: {len(models_data.get('runtime_models', []))}\")\n",
        "    \n",
        "    print(\"\\nüìã Registered Models:\")\n",
        "    for model in models_data.get('models', []):\n",
        "        print(f\"  - {model['model_name']} ({model['model_id']})\")\n",
        "        print(f\"    Type: {model['model_type']}\")\n",
        "        print(f\"    Status: {model['status']}\")\n",
        "        if model.get('base_url'):\n",
        "            print(f\"    Base URL: {model['base_url']}\")\n",
        "    \n",
        "    if models_data.get('runtime_models'):\n",
        "        print(\"\\n‚öôÔ∏è Runtime Models (from environment variables):\")\n",
        "        for model in models_data['runtime_models']:\n",
        "            print(f\"  - {model['model_name']} ({model['model_id']})\")\n",
        "            print(f\"    Type: {model['model_type']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Only Active Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/models\", params={\"include_inactive\": False})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    models_data = response.json()\n",
        "    print(f\"Active models: {models_data['total_models']}\")\n",
        "    for model in models_data.get('models', []):\n",
        "        print(f\"  - {model['model_name']} ({model['model_id']}) - {model['status']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Model by ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get details for a specific model\n",
        "model_id = \"gpt-4-turbo\"  # Replace with an actual model ID from your system\n",
        "response = api_request(\"GET\", f\"/models/{model_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    model = response.json()\n",
        "    print(f\"Model: {model['model_name']}\")\n",
        "    print(f\"ID: {model['model_id']}\")\n",
        "    print(f\"Type: {model['model_type']}\")\n",
        "    print(f\"Description: {model['description']}\")\n",
        "    print(f\"Base URL: {model.get('base_url', 'N/A')}\")\n",
        "    print(f\"Status: {model['status']}\")\n",
        "    \n",
        "    if model.get('capabilities'):\n",
        "        print(f\"\\nCapabilities:\")\n",
        "        caps = model['capabilities']\n",
        "        if caps.get('max_tokens'):\n",
        "            print(f\"  Max tokens: {caps['max_tokens']}\")\n",
        "        if caps.get('context_window'):\n",
        "            print(f\"  Context window: {caps['context_window']}\")\n",
        "        if caps.get('supports_streaming'):\n",
        "            print(f\"  Supports streaming: {caps['supports_streaming']}\")\n",
        "    \n",
        "    if model.get('tags'):\n",
        "        print(f\"\\nTags: {', '.join(model['tags'])}\")\n",
        "elif response.status_code == 404:\n",
        "    print(f\"‚ùå Model '{model_id}' not found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register a New Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Register an OpenAI-compatible model\n",
        "new_model = {\n",
        "    \"model_id\": \"groq-llama-3.1-70b\",\n",
        "    \"model_name\": \"Llama 3.1 70B via Groq\",\n",
        "    \"description\": \"Meta's Llama 3.1 70B model accessed through Groq API\",\n",
        "    \"model_type\": \"openai-compatible\",\n",
        "    \"base_url\": \"https://api.groq.com/openai/v1\",\n",
        "    \"api_key_required\": True,\n",
        "    \"model_path\": \"llama-3.1-70b-versatile\",\n",
        "    \"capabilities\": {\n",
        "        \"max_tokens\": 8192,\n",
        "        \"supports_streaming\": True,\n",
        "        \"supports_function_calling\": True,\n",
        "        \"context_window\": 131072\n",
        "    },\n",
        "    \"config\": {\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"timeout\": 60,\n",
        "        \"retry_attempts\": 3\n",
        "    },\n",
        "    \"status\": \"active\",\n",
        "    \"tags\": [\"groq\", \"llama\", \"openai-compatible\", \"fast\"]\n",
        "}\n",
        "\n",
        "print(\"üìù Registering new model...\")\n",
        "print_json(new_model)\n",
        "\n",
        "response = api_request(\"POST\", \"/models\", json=new_model)\n",
        "\n",
        "if response.status_code == 201:\n",
        "    registered_model = response.json()\n",
        "    print(f\"‚úÖ Model registered successfully!\")\n",
        "    print(f\"Model ID: {registered_model['model_id']}\")\n",
        "    print(f\"Created at: {registered_model.get('created_at', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"‚ùå Failed to register model: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register a vLLM Server Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Register a vLLM server model\n",
        "vllm_model = {\n",
        "    \"model_id\": \"local-llama-2-7b\",\n",
        "    \"model_name\": \"Local Llama 2 7B\",\n",
        "    \"description\": \"Llama 2 7B running on local vLLM server\",\n",
        "    \"model_type\": \"vllm\",\n",
        "    \"base_url\": \"http://localhost:8000\",\n",
        "    \"api_key_required\": False,\n",
        "    \"model_path\": \"/models/llama-2-7b\",\n",
        "    \"capabilities\": {\n",
        "        \"max_tokens\": 4096,\n",
        "        \"supports_streaming\": True,\n",
        "        \"context_window\": 4096\n",
        "    },\n",
        "    \"config\": {\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512,\n",
        "        \"timeout\": 120,\n",
        "        \"retry_attempts\": 2\n",
        "    },\n",
        "    \"status\": \"active\",\n",
        "    \"tags\": [\"vllm\", \"local\", \"llama-2\"]\n",
        "}\n",
        "\n",
        "print(\"üìù Registering vLLM model...\")\n",
        "response = api_request(\"POST\", \"/models\", json=vllm_model)\n",
        "\n",
        "if response.status_code == 201:\n",
        "    print(f\"‚úÖ vLLM model registered: {response.json()['model_id']}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Note: This may fail if the model ID already exists\")\n",
        "    print(f\"Response: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Update a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Update model details\n",
        "model_id = \"groq-llama-3.1-70b\"  # Replace with an actual model ID\n",
        "\n",
        "update_request = {\n",
        "    \"model_name\": \"Llama 3.1 70B (Groq) - Updated\",\n",
        "    \"description\": \"Updated description for Llama 3.1 70B via Groq\",\n",
        "    \"status\": \"active\",\n",
        "    \"tags\": [\"groq\", \"llama\", \"openai-compatible\", \"fast\", \"updated\"]\n",
        "}\n",
        "\n",
        "print(f\"üìù Updating model: {model_id}\")\n",
        "print_json(update_request)\n",
        "\n",
        "response = api_request(\"PUT\", f\"/models/{model_id}\", json=update_request)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    updated_model = response.json()\n",
        "    print(f\"‚úÖ Model updated successfully!\")\n",
        "    print(f\"New name: {updated_model['model_name']}\")\n",
        "    print(f\"Tags: {', '.join(updated_model.get('tags', []))}\")\n",
        "elif response.status_code == 404:\n",
        "    print(f\"‚ùå Model '{model_id}' not found\")\n",
        "else:\n",
        "    print(f\"‚ùå Failed to update model: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Delete a model (runtime models cannot be deleted via API)\n",
        "model_id = \"groq-llama-3.1-70b\"  # Replace with an actual model ID\n",
        "\n",
        "print(f\"üóëÔ∏è Deleting model: {model_id}\")\n",
        "response = api_request(\"DELETE\", f\"/models/{model_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(f\"‚úÖ {result.get('message', 'Model deleted successfully')}\")\n",
        "elif response.status_code == 404:\n",
        "    print(f\"‚ùå Model '{model_id}' not found\")\n",
        "elif response.status_code == 400:\n",
        "    print(f\"‚ùå Cannot delete runtime model (configured via environment variables)\")\n",
        "    print(f\"Response: {response.text}\")\n",
        "else:\n",
        "    print(f\"‚ùå Failed to delete model: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reload Runtime Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reload models configured via environment variables\n",
        "print(\"üîÑ Reloading runtime models from environment variables...\")\n",
        "response = api_request(\"POST\", \"/models/reload\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(f\"‚úÖ {result.get('message', 'Runtime models reloaded successfully')}\")\n",
        "    \n",
        "    # List models again to see any new runtime models\n",
        "    print(\"\\nüìã Updated model list:\")\n",
        "    list_response = api_request(\"GET\", \"/models\")\n",
        "    if list_response.status_code == 200:\n",
        "        models_data = list_response.json()\n",
        "        print(f\"Total models: {models_data['total_models']}\")\n",
        "        print(f\"Runtime models: {len(models_data.get('runtime_models', []))}\")\n",
        "else:\n",
        "    print(f\"‚ùå Failed to reload models: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Evaluation Examples\n",
        "\n",
        "### Single Benchmark Evaluation from Builtin Provider (Simplified API)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Run a single benchmark using the simplified API (Llama Stack compatible)\n",
        "provider_id = \"lm_evaluation_harness\"\n",
        "benchmark_id = \"arc_easy\"\n",
        "\n",
        "single_benchmark_request = {\n",
        "    \"model_name\": \"gpt-4o-mini\",\n",
        "    \"model_configuration\": {\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "    \"timeout_minutes\": 30,\n",
        "    \"retry_attempts\": 1,\n",
        "    \"limit\": 100,  # Limit to 100 samples for faster execution\n",
        "    \"num_fewshot\": 0,\n",
        "    \"experiment_name\": \"Single Benchmark - ARC Easy\",\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"single_benchmark\",\n",
        "        \"provider\": \"lm_evaluation_harness\",\n",
        "        \"benchmark\": \"arc_easy\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìù Creating single benchmark evaluation request...\")\n",
        "print(f\"Provider ID: {provider_id}\")\n",
        "print(f\"Benchmark ID: {benchmark_id}\")\n",
        "print_json(single_benchmark_request)\n",
        "\n",
        "response = api_request(\"POST\", f\"/evaluations/benchmarks/{provider_id}/{benchmark_id}\", json=single_benchmark_request)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    evaluation_response = response.json()\n",
        "    request_id = evaluation_response[\"request_id\"]\n",
        "    print(f\"‚úÖ Single benchmark evaluation created successfully!\")\n",
        "    print(f\"Request ID: {request_id}\")\n",
        "    print(f\"Status: {evaluation_response['status']}\")\n",
        "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to create evaluation\")\n",
        "    print(f\"Error: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Evaluation with Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a simple evaluation request using risk category\n",
        "evaluation_request = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"Simple Risk-Based Evaluation\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"GPT-4 Mini Low Risk Evaluation\",\n",
        "            \"description\": \"Basic evaluation using low risk benchmarks\",\n",
        "            \"model_name\": \"gpt-4o-mini\",\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.0,\n",
        "                \"max_tokens\": 512\n",
        "            },\n",
        "            \"risk_category\": \"low\",\n",
        "            \"timeout_minutes\": 30,\n",
        "            \"retry_attempts\": 1\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"risk_category\",\n",
        "        \"complexity\": \"simple\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìù Creating evaluation request...\")\n",
        "print_json(evaluation_request)\n",
        "\n",
        "response = api_request(\"POST\", \"/evaluations\", json=evaluation_request)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    evaluation_response = response.json()\n",
        "    request_id = evaluation_response[\"request_id\"]\n",
        "    print(f\"‚úÖ Evaluation created successfully!\")\n",
        "    print(f\"Request ID: {request_id}\")\n",
        "    print(f\"Status: {evaluation_response['status']}\")\n",
        "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to create evaluation\")\n",
        "    print(f\"Error: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation with Explicit Backend Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create an evaluation with explicit backend configuration\n",
        "explicit_evaluation = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"Explicit Backend Configuration\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"LM-Eval Harness Evaluation\",\n",
        "            \"description\": \"Evaluation with explicit lm-evaluation-harness configuration\",\n",
        "            \"model_name\": \"gpt-4o-mini\",\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.1,\n",
        "                \"max_tokens\": 256,\n",
        "                \"top_p\": 0.95\n",
        "            },\n",
        "            \"backends\": [\n",
        "                {\n",
        "                    \"name\": \"lm-eval-backend\",\n",
        "                    \"type\": \"lm-evaluation-harness\",\n",
        "                    \"config\": {\n",
        "                        \"batch_size\": 1,\n",
        "                        \"device\": \"cpu\"\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"arc_easy\",\n",
        "                            \"tasks\": [\"arc_easy\"],\n",
        "                            \"config\": {\n",
        "                                \"num_fewshot\": 5,\n",
        "                                \"limit\": 50\n",
        "                            }\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"hellaswag\",\n",
        "                            \"tasks\": [\"hellaswag\"],\n",
        "                            \"config\": {\n",
        "                                \"num_fewshot\": 10,\n",
        "                                \"limit\": 100\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"timeout_minutes\": 45,\n",
        "            \"retry_attempts\": 2\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"explicit_backend\",\n",
        "        \"complexity\": \"intermediate\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìù Creating evaluation with explicit backend...\")\n",
        "response = api_request(\"POST\", \"/evaluations\", json=explicit_evaluation)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    explicit_response = response.json()\n",
        "    explicit_request_id = explicit_response[\"request_id\"]\n",
        "    print(f\"‚úÖ Explicit evaluation created!\")\n",
        "    print(f\"Request ID: {explicit_request_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NeMo Evaluator Integration\n",
        "\n",
        "### Single NeMo Evaluator Container"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example with single NeMo Evaluator container\n",
        "nemo_single_evaluation = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"NeMo Evaluator Single Container\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"GPT-4 via NeMo Evaluator\",\n",
        "            \"description\": \"Remote evaluation using NeMo Evaluator container\",\n",
        "            \"model_name\": \"gpt-4-turbo\",\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.0,\n",
        "                \"max_tokens\": 512,\n",
        "                \"top_p\": 0.95\n",
        "            },\n",
        "            \"backends\": [\n",
        "                {\n",
        "                    \"name\": \"remote-nemo-evaluator\",\n",
        "                    \"type\": \"nemo-evaluator\",\n",
        "                    \"config\": {\n",
        "                        \"endpoint\": \"localhost\",\n",
        "                        \"port\": 3825,\n",
        "                        \"model_endpoint\": \"https://api.openai.com/v1/chat/completions\",\n",
        "                        \"endpoint_type\": \"chat\",\n",
        "                        \"api_key_env\": \"OPENAI_API_KEY\",\n",
        "                        \"timeout_seconds\": 1800,\n",
        "                        \"max_retries\": 2,\n",
        "                        \"verify_ssl\": False,\n",
        "                        \"framework_name\": \"eval-hub-example\",\n",
        "                        \"parallelism\": 1,\n",
        "                        \"limit_samples\": 25,\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"top_p\": 0.95\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"mmlu_pro_sample\",\n",
        "                            \"tasks\": [\"mmlu_pro\"],\n",
        "                            \"config\": {\n",
        "                                \"limit\": 25,\n",
        "                                \"num_fewshot\": 5\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"timeout_minutes\": 60,\n",
        "            \"retry_attempts\": 1\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"nemo_evaluator_single\",\n",
        "        \"complexity\": \"advanced\",\n",
        "        \"backend\": \"remote_container\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìù Creating NeMo Evaluator evaluation...\")\n",
        "print(\"Note: This requires a running NeMo Evaluator container on localhost:3825\")\n",
        "\n",
        "response = api_request(\"POST\", \"/evaluations\", json=nemo_single_evaluation)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    nemo_response = response.json()\n",
        "    nemo_request_id = nemo_response[\"request_id\"]\n",
        "    print(f\"‚úÖ NeMo evaluation created!\")\n",
        "    print(f\"Request ID: {nemo_request_id}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NeMo evaluation failed (container may not be running)\")\n",
        "    print(f\"Response: {response.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Container NeMo Evaluator Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example with multiple specialized NeMo Evaluator containers\n",
        "nemo_multi_evaluation = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"Multi-Container NeMo Evaluation\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"Distributed LLaMA Evaluation\",\n",
        "            \"description\": \"Multi-container evaluation across specialized endpoints\",\n",
        "            \"model_name\": \"llama-3.1-8b\",\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.1,\n",
        "                \"max_tokens\": 512,\n",
        "                \"top_p\": 0.95\n",
        "            },\n",
        "            \"backends\": [\n",
        "                {\n",
        "                    \"name\": \"academic-evaluator\",\n",
        "                    \"type\": \"nemo-evaluator\",\n",
        "                    \"config\": {\n",
        "                        \"endpoint\": \"academic-eval.example.com\",\n",
        "                        \"port\": 3825,\n",
        "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                        \"endpoint_type\": \"chat\",\n",
        "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
        "                        \"timeout_seconds\": 3600,\n",
        "                        \"framework_name\": \"eval-hub-academic\",\n",
        "                        \"parallelism\": 2\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"mmlu_pro\",\n",
        "                            \"tasks\": [\"mmlu_pro\"],\n",
        "                            \"config\": {\"limit\": 100, \"num_fewshot\": 5}\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"arc_challenge\",\n",
        "                            \"tasks\": [\"arc_challenge\"],\n",
        "                            \"config\": {\"limit\": 200, \"num_fewshot\": 25}\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"math-evaluator\",\n",
        "                    \"type\": \"nemo-evaluator\",\n",
        "                    \"config\": {\n",
        "                        \"endpoint\": \"math-eval.example.com\",\n",
        "                        \"port\": 3825,\n",
        "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                        \"endpoint_type\": \"chat\",\n",
        "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"parallelism\": 1,\n",
        "                        \"framework_name\": \"eval-hub-math\"\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"gsm8k\",\n",
        "                            \"tasks\": [\"gsm8k\"],\n",
        "                            \"config\": {\"limit\": 100, \"num_fewshot\": 8}\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"math\",\n",
        "                            \"tasks\": [\"hendrycks_math\"],\n",
        "                            \"config\": {\"limit\": 50, \"num_fewshot\": 4}\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"timeout_minutes\": 120,\n",
        "            \"retry_attempts\": 2\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"nemo_evaluator_multi\",\n",
        "        \"complexity\": \"expert\",\n",
        "        \"backend\": \"distributed_containers\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìù Creating multi-container NeMo evaluation...\")\n",
        "print(\"Note: This is a hypothetical example with multiple remote containers\")\n",
        "print_json(nemo_multi_evaluation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Status Monitoring\n",
        "\n",
        "### Check Evaluation Status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to check evaluation status\n",
        "def check_evaluation_status(request_id: str):\n",
        "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        status_data = response.json()\n",
        "        print(f\"üìä Evaluation Status for {request_id}\")\n",
        "        print(f\"Status: {status_data['status']}\")\n",
        "        print(f\"Progress: {status_data.get('progress_percentage', 0):.1f}%\")\n",
        "        print(f\"Total evaluations: {status_data.get('total_evaluations', 0)}\")\n",
        "        print(f\"Completed: {status_data.get('completed_evaluations', 0)}\")\n",
        "        print(f\"Failed: {status_data.get('failed_evaluations', 0)}\")\n",
        "\n",
        "        if status_data.get('results'):\n",
        "            print(f\"Results available: {len(status_data['results'])}\")\n",
        "\n",
        "        return status_data\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to get status: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Check status of previously created evaluations (if they exist)\n",
        "try:\n",
        "    if 'request_id' in locals():\n",
        "        check_evaluation_status(request_id)\n",
        "except NameError:\n",
        "    print(\"No evaluation request_id available to check\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor Evaluation Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to monitor evaluation until completion\n",
        "def monitor_evaluation(request_id: str, max_wait_time: int = 300):\n",
        "    \"\"\"Monitor an evaluation until completion or timeout.\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    while time.time() - start_time < max_wait_time:\n",
        "        status_data = check_evaluation_status(request_id)\n",
        "\n",
        "        if not status_data:\n",
        "            break\n",
        "\n",
        "        status = status_data['status']\n",
        "\n",
        "        if status in ['completed', 'failed', 'cancelled']:\n",
        "            print(f\"üèÅ Evaluation {status}!\")\n",
        "\n",
        "            if status == 'completed' and status_data.get('results'):\n",
        "                print(\"\\nüìä Results Summary:\")\n",
        "                for result in status_data['results'][:3]:  # Show first 3 results\n",
        "                    print(f\"  - {result['benchmark_name']}: {result['status']}\")\n",
        "                    if result.get('metrics'):\n",
        "                        for metric, value in list(result['metrics'].items())[:2]:\n",
        "                            print(f\"    {metric}: {value}\")\n",
        "\n",
        "            return status_data\n",
        "\n",
        "        print(f\"‚è≥ Still {status}, waiting...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "    print(f\"‚è∞ Monitoring timed out after {max_wait_time} seconds\")\n",
        "    return None\n",
        "\n",
        "# Example usage (uncomment if you have a running evaluation)\n",
        "# monitor_evaluation(request_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List All Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/evaluations\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    evaluations = response.json()\n",
        "    print(f\"üìã Active evaluations: {len(evaluations)}\")\n",
        "\n",
        "    for eval_resp in evaluations:\n",
        "        print(f\"\\nüîç {eval_resp['request_id']}\")\n",
        "        print(f\"   Status: {eval_resp['status']}\")\n",
        "        print(f\"   Progress: {eval_resp.get('progress_percentage', 0):.1f}%\")\n",
        "        print(f\"   Created: {eval_resp['created_at']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## System Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = api_request(\"GET\", \"/metrics/system\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    metrics = response.json()\n",
        "    print(\"üìä System Metrics:\")\n",
        "    print(f\"  Active evaluations: {metrics['active_evaluations']}\")\n",
        "    print(f\"  Running tasks: {metrics['running_tasks']}\")\n",
        "    print(f\"  Total requests: {metrics['total_requests']}\")\n",
        "\n",
        "    if metrics.get('status_breakdown'):\n",
        "        print(\"\\n  Status breakdown:\")\n",
        "        for status, count in metrics['status_breakdown'].items():\n",
        "            print(f\"    {status}: {count}\")\n",
        "\n",
        "    if metrics.get('memory_usage'):\n",
        "        print(f\"\\n  Memory usage:\")\n",
        "        print(f\"    Active evaluations: {metrics['memory_usage']['active_evaluations_mb']:.1f} MB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Management\n",
        "\n",
        "### Cancel an Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to cancel an evaluation\n",
        "def cancel_evaluation(request_id: str):\n",
        "    response = api_request(\"DELETE\", f\"/evaluations/{request_id}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"‚úÖ {result['message']}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to cancel: {response.text}\")\n",
        "        return False\n",
        "\n",
        "# Example usage (uncomment if you want to cancel an evaluation)\n",
        "# cancel_evaluation(request_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling Examples\n",
        "\n",
        "### Invalid Request Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of invalid request to demonstrate error handling\n",
        "invalid_request = {\n",
        "    \"request_id\": \"invalid-uuid-format\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"\",  # Invalid: empty name\n",
        "            \"model_name\": \"\",  # Invalid: empty model name\n",
        "            \"backends\": []  # Invalid: no backends\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üìù Testing error handling with invalid request...\")\n",
        "response = api_request(\"POST\", \"/evaluations\", json=invalid_request)\n",
        "\n",
        "if response.status_code >= 400:\n",
        "    print(\"‚úÖ Error handling working correctly\")\n",
        "    error_data = response.json()\n",
        "    print(f\"Error type: {response.status_code}\")\n",
        "    print(f\"Error message: {error_data.get('detail', 'Unknown error')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Non-existent Resource Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test accessing non-existent evaluation\n",
        "fake_request_id = str(uuid4())\n",
        "print(f\"üîç Testing access to non-existent evaluation: {fake_request_id}\")\n",
        "\n",
        "response = api_request(\"GET\", f\"/evaluations/{fake_request_id}\")\n",
        "\n",
        "if response.status_code == 404:\n",
        "    print(\"‚úÖ 404 handling working correctly\")\n",
        "    error_data = response.json()\n",
        "    print(f\"Error: {error_data['detail']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Examples\n",
        "\n",
        "### Batch Evaluation Requests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create multiple evaluations for comparison\n",
        "batch_requests = []\n",
        "\n",
        "models_to_compare = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
        "risk_levels = [\"low\", \"medium\"]\n",
        "\n",
        "for model in models_to_compare:\n",
        "    for risk in risk_levels:\n",
        "        batch_request = {\n",
        "            \"request_id\": str(uuid4()),\n",
        "            \"experiment_name\": f\"Batch Comparison - {model} - {risk} risk\",\n",
        "            \"evaluations\": [\n",
        "                {\n",
        "                    \"name\": f\"{model} {risk} risk evaluation\",\n",
        "                    \"model_name\": model,\n",
        "                    \"model_configuration\": {\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"max_tokens\": 256\n",
        "                    },\n",
        "                    \"risk_category\": risk,\n",
        "                    \"timeout_minutes\": 30\n",
        "                }\n",
        "            ],\n",
        "            \"tags\": {\n",
        "                \"batch_id\": \"model_comparison_001\",\n",
        "                \"model\": model,\n",
        "                \"risk_level\": risk\n",
        "            }\n",
        "        }\n",
        "        batch_requests.append(batch_request)\n",
        "\n",
        "print(f\"üì¶ Creating {len(batch_requests)} batch evaluations...\")\n",
        "\n",
        "batch_results = []\n",
        "for i, request in enumerate(batch_requests):\n",
        "    print(f\"\\nüìù Creating batch request {i+1}/{len(batch_requests)}\")\n",
        "    response = api_request(\"POST\", \"/evaluations\", json=request)\n",
        "\n",
        "    if response.status_code == 202:\n",
        "        batch_results.append(response.json())\n",
        "        print(f\"‚úÖ Batch {i+1} created: {response.json()['request_id']}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Batch {i+1} failed\")\n",
        "\n",
        "print(f\"\\nüìä Successfully created {len(batch_results)} batch evaluations\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test various configuration combinations\n",
        "test_configs = [\n",
        "    {\n",
        "        \"name\": \"High timeout test\",\n",
        "        \"config\": {\"timeout_minutes\": 120, \"retry_attempts\": 5},\n",
        "        \"expected\": \"success\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Zero timeout test\",\n",
        "        \"config\": {\"timeout_minutes\": 0, \"retry_attempts\": 1},\n",
        "        \"expected\": \"validation_error\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Negative retry test\",\n",
        "        \"config\": {\"timeout_minutes\": 30, \"retry_attempts\": -1},\n",
        "        \"expected\": \"validation_error\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for test in test_configs:\n",
        "    print(f\"\\nüß™ Testing: {test['name']}\")\n",
        "\n",
        "    test_request = {\n",
        "        \"request_id\": str(uuid4()),\n",
        "        \"experiment_name\": test['name'],\n",
        "        \"evaluations\": [\n",
        "            {\n",
        "                \"name\": \"Config test\",\n",
        "                \"model_name\": \"gpt-4o-mini\",\n",
        "                \"risk_category\": \"low\",\n",
        "                **test['config']\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = api_request(\"POST\", \"/evaluations\", json=test_request)\n",
        "\n",
        "    if test['expected'] == \"success\" and response.status_code == 202:\n",
        "        print(\"‚úÖ Test passed\")\n",
        "    elif test['expected'] == \"validation_error\" and response.status_code >= 400:\n",
        "        print(\"‚úÖ Validation correctly rejected invalid config\")\n",
        "    else:\n",
        "        print(f\"‚ùå Unexpected result: {response.status_code}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated comprehensive usage of the Eval Hub API including:\n",
        "\n",
        "- ‚úÖ **Basic Operations**: Health checks, provider/benchmark discovery\n",
        "- ‚úÖ **Model Management**: Register, list, update, and delete models\n",
        "- ‚úÖ **Simple Evaluations**: Risk category-based evaluations\n",
        "- ‚úÖ **Advanced Evaluations**: Explicit backend configuration\n",
        "- ‚úÖ **NeMo Integration**: Single and multi-container setups\n",
        "- ‚úÖ **Monitoring**: Status checking and progress tracking\n",
        "- ‚úÖ **Management**: Cancellation and system metrics\n",
        "- ‚úÖ **Error Handling**: Validation and error responses\n",
        "- ‚úÖ **Batch Operations**: Multiple evaluation management\n",
        "\n",
        "For production use, remember to:\n",
        "- Use proper API keys and authentication\n",
        "- Configure appropriate timeouts for your evaluation complexity\n",
        "- Monitor resource usage and system metrics\n",
        "- Handle errors gracefully in your applications\n",
        "- Use the async evaluation mode for long-running evaluations\n",
        "\n",
        "The Eval Hub provides a powerful and flexible API for orchestrating machine learning model evaluations across multiple backends and evaluation frameworks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/rui/Sync/code/experiments/features/eval-service/poc/eval-hub/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}